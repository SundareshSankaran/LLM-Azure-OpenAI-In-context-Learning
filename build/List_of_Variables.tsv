	variable	description	type	value
0	params	Parameters	page	
1	section_input_params	Parameters	section	
2	inputData	Select input data:	inputtable	
3	textCol	Select text column:	columnselector	
4	section_prompts	Prompts	section	
5	_systemPrompt	Provide system prompt:	textarea	
6	text_system_prompt		text	Use the system prompt to provide broad instructions to the LLM such as role, task description and response specification.
7	userPrompt	Provide user prompt:	textarea	
8	text_user_prompt		text	Use the user prompt to provide specific instructions on the task to perform.  Use tags like {Question}, {Context} etc. to refer to the context provided.
9	userExample	Provide illustrative example(s):	textarea	
10	text_user_example		text	Use the user example to provide 0, 1, or more illustrative examples of context and desired response from the LLM. Tag examples as Example: and Answer:.
11	section_output_specs	Output specification	section	
12	add_q_y_n		checkbox	
13	temperature	Select temperature for output:	numstepper	
14	text_temperature		text	Use the temperature control to govern how the LLM generates its response based on next word prediction.
15	topP	Select top P for output:	numstepper	
16	text_topP		text	Similar to temperature, this controls how deterministic the model is. Lowering Top P will narrow the modelâ€™s token selection to likelier tokens. Increasing Top P will let the model choose from tokens with both high and low likelihood. Try adjusting temperature or Top P but not both.
17	maxTokens	Select the max number of tokens for output:	numstepper	
18	text_maxTokens		text	Use the max token control to limit the maximum number of tokens per the response. 1 token is approximately 4 characters of English text.
19	frequencyPenalty	Select the frequency penalty:	numstepper	
20	text_frequencyPenalty		text	The frequency penalty reduces the likelihood of repeating words by applying a penalty to tokens based on how often they have already appeared in the text. A higher frequency penalty decreases repetition by discouraging the model from generating the same words multiple times.
21	presencePenalty	Select the presence penalty:	numstepper	
22	text_presencePenalty		text	The presence penalty reduces the chance of repeating any word that has already appeared in the text, encouraging the model to introduce new topics. Unlike the frequency penalty, it applies a uniform penalty to all repeated tokens, regardless of how often they appear.
23	outputTable	Provide output table:	outputtable	
24	config	Configuration	page	
25	section_gen_model	Text generation model	section	
26	genModelDeployment	Provide name of your Azure OpenAI generation model deployment:	textfield	
27	section_azure_openai	Azure OpenAI service:	section	
28	azureKeyLocation	Provide path to your Azure OpenAI key:	path	
29	text_key_details		text	Ensure this key is located in a file saved in a secure folder.
30	azureOpenAIEndpoint	Provide URL for Azure OpenAI service endpoint:	textfield	
31	azureRegion	Provide region for Azure OpenAI service	textfield	
32	openAIVersion	OpenAI API Version	textfield	
33	about	About	page	
34	about_description		text	LLM - Azure OpenAI In-context Learning==============================================This custom step helps you interact with a Large Language Model (LLM) calling an [Azure OpenAI](https://microsoftlearning.github.io/mslearn-openai/Instructions/Exercises/01-get-started-azure-openai.html) service to process simple instructions on specified input data. It uses an approach called In-context learning which uses provided examples to perform a task.  If no example is provided, then the LLM simply uses the provided context.  This is useful for cases where a call to an LLM does not require prior search, filter or query of data sources (such as what Retrieval Augmented Generation provides) . Run inside a SAS session, this custom step takes either a SAS dataset or a CAS table as input and returns a SAS dataset (or CAS table) as output, with the response added as a new variable.
35	section_prereqs	Prerequisites	section	
36	text_prereqs		text	1. Python is available to the SAS Viya Compute session.  2. Python packages to be installed:     i.  openai: https://pypi.org/project/openai/     ii.  pandas: https://pypi.org/project/pandas/3. Viya 4 environment version 2025.01 or later 4. Valid Azure OpenAI service with large language models deployed.  Refer here for instructions: https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio
37	about_parameters	Parameters	section	
38	parameters_input	Input parameters	section	
39	input_parameters_text		text	"1. Input table (input port, required): Attach either a SAS dataset or a SAS Cloud Analytics Services (CAS) table to the input port of this step.2. Select text column (column selector, max 1): Select a column containing the context to be provided to the LLM3. System prompt (text area): Provide a system prompt for use by the LLM. The system prompt helps clarify the role that the LLM plays and is also used for other instructions such as how to answer the question and the format in which to present the results.4. User prompt (text area): Provide a user prompt which contains specific instructions to be fulfilled by the LLM.  In the user prompt, use fields like {Question} and {Context} to refer to the question or context provided.5. Illustrative examples (text area):  Provide some illustrative examples (the 'shots') which are used as a basis for the LLM to answer the question. Provide this in the form of ""Question:"" and ""Answer:"" pairs"
40	parameters_output_specs	Output specifications	section	
41	output_parameters_text		text	1. Temperature (numeric stepper): Use the temperature control to govern how the LLM generates its response based on next word prediction.2. Output table (output port, required) : Attach a table referring to either a SAS dataset or CAS table which contains the original input columns along with the LLM's answer.
42	parameters_config	Configuration 	section	
43	output_parameters_text_1		text	1. Text generation model (text field): Provide the name of a Azure OpenAI model deployment.For convenience, you may choose to use the same name as the OpenAI LLM. Example, gpt-35-turbo to gpt-35-turbo.2. Azure Key Location (file selector): Provide path to your Azure OpenAI key, located on the filesystem.  This would be a text file containing the value of the key.  Make sure this is saved in a secure location.3. Azure OpenAI endpoint (text field): Provide the URL of the Azure OpenAI service.4. Azure OpenAI region (text field): Provide the region where the service is set up (e.g. eastus2)5. OpenAI version (default provided): Provide an OpenAI version number in case you want to override the default.
44	section_assumptions	Assumptions	section	
45	text_assumptions		text	Current assumptions for this initial versions (future versions may improve upon the same):1. Users  choose either a SAS dataset or Cloud Analytics Services (CAS) table as their input 2. User has already configured Azure OpenAI to deploy both an embedding function and LLM service, or knows the deployment names.
46	about_runtimecontrol	Run-time Control	section	
47	runtimecontrol_text		text	Note: Run-time control is optional.  You may choose whether to execute the main code of this step or not, based on upstream conditions set by earlier SAS programs.  This includes nodes run prior to this custom step earlier in a SAS Studio Flow, or a previous program in the same session.Refer this blog (https://communities.sas.com/t5/SAS-Communities-Library/Switch-on-switch-off-run-time-control-of-SAS-Studio-Custom-Steps/ta-p/885526) for more details on the concept.The following macro variable,```sas_aicl_run_trigger```will initialize with a value of 1 by default, indicating an 'enabled' status and allowing the custom step to run.If you wish to control execution of this custom step, include code in an upstream SAS program to set this variable to 0.  This 'disables' execution of the custom step.To 'disable' this step, run the following code upstream:```sas%global _aicl_run_trigger;%let _aicl_run_trigger = 0;```To 'enable' this step again, run the following (it's assumed that this has already been set as a global variable):```sas%let _aicl_run_trigger = 1;```IMPORTANT: Be aware that disabling this step means that none of its main execution code will run, and any  downstream code which was dependent on this code may fail.  Change this setting only if it aligns with the objective of your SAS Studio program.
48	about_documentation	Documentation	section	
49	documentation_text		text	1.  Azure OpenAI service: https://learn.microsoft.com/en-us/azure/ai-services/openai/2.  SAS Communities article on configuring Viya for Python integration: https://communities.sas.com/t5/SAS-Communities-Library/Configuring-SAS-Viya-for-Python-Integration/ta-p/8474593. The SAS Viya Platform Deployment Guide (refer to SAS Configurator for Open Source within): https://go.documentation.sas.com/doc/en/itopscdc/default/itopssr/p1n66p7u2cm8fjn13yeggzbxcqqg.htm?fromDefault=#p19cpvrrjw3lurn135ih46tjm7oi 4. OpenAI API versions change periodically. Keep track of them here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
50	version_text		text	Version: 1.0  (13MAR2025)
51	contact_text		text	Created/contact: - Sundaresh Sankaran (sundaresh.sankaran@sas.com) - Crystal Baker (crystal.baker@sas.com)
