{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_sas_studio_custom_steps import CustomStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CustomStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CustomStep in module py_sas_studio_custom_steps.custom_step object:\n",
      "\n",
      "class CustomStep(builtins.object)\n",
      " |  CustomStep(custom_step_file=None, name=None, creationTimeStamp=None, modifiedTimeStamp=None, createdBy=None, modifiedBy=None, displayName=None, localDisplayName=None, properties=None, links=None, metadataVersion=None, version=None, type=None, flowMetadata=None, ui=None, templates=None) -> None\n",
      " |  \n",
      " |  This class helps you perform operations on a SAS Studio Custom Step programmatically\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, custom_step_file=None, name=None, creationTimeStamp=None, modifiedTimeStamp=None, createdBy=None, modifiedBy=None, displayName=None, localDisplayName=None, properties=None, links=None, metadataVersion=None, version=None, type=None, flowMetadata=None, ui=None, templates=None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  create_custom_step(self, custom_step_path)\n",
      " |      This function writes a CustomStep object to a SAS Studio Custom Step file at a desired path.\n",
      " |  \n",
      " |  extract_sas_program(self, custom_step_file)\n",
      " |      This function extracts and returns the SAS program portion of a custom step file.  Provide the full path to the custom step as an argument.\n",
      " |  \n",
      " |  get_pages(self)\n",
      " |      This function returns all pages provided in a CustomStep object. Introduced v0.3.3\n",
      " |  \n",
      " |  list_keys(self)\n",
      " |      This function lists and returns all keys forming part of a CustomStep object.\n",
      " |  \n",
      " |  load_step_file(self, custom_step_file)\n",
      " |      This functions loads a custom step object with attributes contained in a custom step file\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Auto_Generated_c673e432-415c-4b4e-beba-6a2806364744',\n",
       " 'creationTimeStamp': None,\n",
       " 'modifiedTimeStamp': None,\n",
       " 'createdBy': None,\n",
       " 'modifiedBy': None,\n",
       " 'displayName': None,\n",
       " 'localDisplayName': None,\n",
       " 'properties': None,\n",
       " 'links': None,\n",
       " 'metadataVersion': None,\n",
       " 'version': None,\n",
       " 'type': None,\n",
       " 'flowMetadata': None,\n",
       " 'ui': None,\n",
       " 'templates': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs[\"ui\"]=\"foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Auto_Generated_c673e432-415c-4b4e-beba-6a2806364744',\n",
       " 'creationTimeStamp': None,\n",
       " 'modifiedTimeStamp': None,\n",
       " 'createdBy': None,\n",
       " 'modifiedBy': None,\n",
       " 'displayName': None,\n",
       " 'localDisplayName': None,\n",
       " 'properties': None,\n",
       " 'links': None,\n",
       " 'metadataVersion': None,\n",
       " 'version': None,\n",
       " 'type': None,\n",
       " 'flowMetadata': None,\n",
       " 'ui': 'foo',\n",
       " 'templates': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"components.json\",\"r\") as f:\n",
    "    js = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'showPageContentOnly': True,\n",
       " 'pages': [{'id': 'params',\n",
       "   'type': 'page',\n",
       "   'label': 'Parameters',\n",
       "   'children': [{'id': 'section_input_params',\n",
       "     'type': 'section',\n",
       "     'label': 'Parameters',\n",
       "     'open': True,\n",
       "     'children': [{'id': 'inputData',\n",
       "       'type': 'inputtable',\n",
       "       'label': 'Select input data',\n",
       "       'required': True,\n",
       "       'placeholder': '',\n",
       "       'visible': ''},\n",
       "      {'id': 'textCol',\n",
       "       'type': 'columnselector',\n",
       "       'label': 'Select text column containing instructions and context:',\n",
       "       'include': None,\n",
       "       'order': False,\n",
       "       'columntype': 'a',\n",
       "       'max': None,\n",
       "       'min': None,\n",
       "       'visible': '',\n",
       "       'table': None},\n",
       "      {'id': 'docId',\n",
       "       'type': 'columnselector',\n",
       "       'label': 'Select ID column:',\n",
       "       'include': None,\n",
       "       'order': False,\n",
       "       'columntype': 'a',\n",
       "       'max': None,\n",
       "       'min': None,\n",
       "       'visible': '',\n",
       "       'table': None},\n",
       "      {'id': 'copyVarList',\n",
       "       'type': 'columnselector',\n",
       "       'label': 'Select additional columns to copy over:',\n",
       "       'include': None,\n",
       "       'order': False,\n",
       "       'columntype': 'a',\n",
       "       'max': None,\n",
       "       'min': None,\n",
       "       'visible': '',\n",
       "       'table': None},\n",
       "      {'id': 'section_prompts',\n",
       "       'type': 'section',\n",
       "       'label': 'Prompts',\n",
       "       'open': True,\n",
       "       'children': [{'id': 'systemPrompt',\n",
       "         'type': 'textarea',\n",
       "         'label': 'Provide system prompt:',\n",
       "         'placeholder': '',\n",
       "         'required': False,\n",
       "         'visible': ''},\n",
       "        {'id': 'userPrompt',\n",
       "         'type': 'textarea',\n",
       "         'label': 'Provide user prompt:',\n",
       "         'placeholder': '',\n",
       "         'required': False,\n",
       "         'visible': ''}]}]},\n",
       "    {'id': 'section_output_specs',\n",
       "     'type': 'section',\n",
       "     'label': 'Output specification',\n",
       "     'open': True,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'temperature',\n",
       "       'type': 'numstepper',\n",
       "       'label': 'Select temperature for output:',\n",
       "       'required': False,\n",
       "       'integer': False,\n",
       "       'min': None,\n",
       "       'max': None,\n",
       "       'stepsize': 1},\n",
       "      {'id': 'outputTable',\n",
       "       'type': 'outputtable',\n",
       "       'label': 'Provide output table:',\n",
       "       'required': True,\n",
       "       'placeholder': '',\n",
       "       'visible': ''}]}]},\n",
       "  {'id': 'config',\n",
       "   'type': 'page',\n",
       "   'label': 'Configuration',\n",
       "   'children': [{'id': 'embedding_model',\n",
       "     'type': 'section',\n",
       "     'label': 'Embedding model',\n",
       "     'open': True,\n",
       "     'children': [{'id': 'embeddingModelDeployment',\n",
       "       'type': 'textfield',\n",
       "       'label': 'Provide name of your Azure OpenAI model deployment for embedding models: ',\n",
       "       'placeholder': '',\n",
       "       'required': True,\n",
       "       'visible': ''}]},\n",
       "    {'id': 'section5',\n",
       "     'type': 'section',\n",
       "     'label': 'Text generation model',\n",
       "     'open': False,\n",
       "     'children': [{'id': 'genModelDeployment',\n",
       "       'type': 'textfield',\n",
       "       'label': 'Provide name of your Azure OpenAI generation model deployment:',\n",
       "       'placeholder': '',\n",
       "       'required': True,\n",
       "       'visible': ''}]},\n",
       "    {'id': 'section_azure_openai',\n",
       "     'type': 'section',\n",
       "     'label': 'Azure OpenAI service:',\n",
       "     'open': True,\n",
       "     'children': [{'id': 'azureKeyLocation',\n",
       "       'type': 'path',\n",
       "       'label': 'Provide path to your Azure OpenAI key:',\n",
       "       'pathtype': 'file',\n",
       "       'placeholder': '',\n",
       "       'required': False,\n",
       "       'visible': ''},\n",
       "      {'id': 'text_key_details',\n",
       "       'type': 'text',\n",
       "       'text': 'Ensure this key is located in a file saved in a secure folder.',\n",
       "       'visible': ''},\n",
       "      {'id': 'azureOpenAIEndpoint',\n",
       "       'type': 'textfield',\n",
       "       'label': 'Provide URL for Azure OpenAI service endpoint:',\n",
       "       'placeholder': '',\n",
       "       'required': True,\n",
       "       'visible': ''},\n",
       "      {'id': 'azureRegion',\n",
       "       'type': 'textfield',\n",
       "       'label': 'Provide region for Azure OpenAI service',\n",
       "       'placeholder': 'Default value: eastus2',\n",
       "       'required': False,\n",
       "       'visible': ''},\n",
       "      {'id': 'openAIVersion',\n",
       "       'type': 'textfield',\n",
       "       'label': 'OpenAI API Version',\n",
       "       'placeholder': '2024-10-21',\n",
       "       'required': False,\n",
       "       'visible': ''}]}]},\n",
       "  {'id': 'about',\n",
       "   'type': 'page',\n",
       "   'label': 'About',\n",
       "   'children': [{'id': 'about_description',\n",
       "     'type': 'text',\n",
       "     'text': \"LLM - Azure OpenAI-based Retrieval Augmented Generation (RAG) \\n==============================================\\n\\nThis custom step uses a Retrieval Augmented Generation (RAG) approach to provide right context to an Azure OpenAI Large Language Model (LLM) for answering a question.  \\n\\nLLMs require relevant context to provide useful answers, especially for questions based on a local corpus of knowledge.  \\n\\nA RAG approach, explained in simple terms, retrieves relevant data from a knowledge base and provides the same to an LLM to use as context.  RAG-based are expected to reduce LLM hallucinations (i.e. an LLM provides irrelevant or false answers).  This custom step implements RAG with a Chroma DB vector store and passes retrieved documents to an Azure OpenAI service.   \\n\\n**IMPORTANT:** Be aware that this custom step uses an Azure OpenAI service that results in data being sent over to the service.  Ensure you use this only in accordance with your organization's policies on calling external LLMs.\",\n",
       "     'visible': ''},\n",
       "    {'id': 'section_prereqs',\n",
       "     'type': 'section',\n",
       "     'label': 'Prerequisites',\n",
       "     'open': False,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'text_prereqs',\n",
       "       'type': 'text',\n",
       "       'text': '1. Python:  Python version 3.10 is recommended to avoid package support or dependency issues.\\n\\n2. Python packages to be installed:\\n\\n   i.    langchain: https://pypi.org/project/langchain/\\n   ii.   langchain-community: https://pypi.org/project/langchain-community/\\n   iii.  langchain-openai: https://pypi.org/project/langchain-openai/\\n  iv.   PyPDF: https://pypi.org/project/pypdf/\\n  v.    sentence-transformers: https://pypi.org/project/sentence-transformers/\\n  vi.   chromadb: https://pypi.org/project/chromadb/\\n  vii.  pysqlite-binary: https://pypi.org/project/pysqlite-binary/\\n\\n3. Viya 4 environment version 2024.01 or later\\n\\n4. Valid Azure OpenAI service with embedding & large language models deployed.  Refer here for instructions: https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio \\n',\n",
       "       'visible': ''}]},\n",
       "    {'id': 'section_assumptions',\n",
       "     'type': 'section',\n",
       "     'label': 'Assumptions',\n",
       "     'open': False,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'text_assumptions',\n",
       "       'type': 'text',\n",
       "       'text': 'Current assumptions for this initial versions (future versions may improve upon the same):\\n\\n1. Users  choose either an existing Chroma DB vector database collection or load PDF,  SAS dataset, pandas DataFrame or CSV files to an existing or new Chroma DB collection.\\n\\n2. Users may load all PDFs in a directory on the SAS Server (filesystem), or select a PDF/sas7bdat/DataFrame/CSV of their choice.\\n\\n3. The code assumes use of a Chroma DB vector store.  Users may choose to replace this with other supported vector stores.\\n\\n4. The code uses the langchain LLM framework.  \\n\\n5. PDFs (containing text), CSV, SAS datasets and pandas DataFrames are currently the only loadable file format allowed.  Users are however free to ingest various other document types into a Chroma DB collection beforehand, using the \"Vector Databases - Hydrate Chroma DB collection\" SAS Studio Custom Step (refer documentation)\\n\\n6. User has already configured Azure OpenAI to deploy both an embedding function and LLM service, or knows the deployment names. \\n',\n",
       "       'visible': ''}]},\n",
       "    {'id': 'about_parameters',\n",
       "     'type': 'section',\n",
       "     'label': 'Parameters',\n",
       "     'open': True,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'parameters_input',\n",
       "       'type': 'section',\n",
       "       'label': 'Input parameters',\n",
       "       'open': True,\n",
       "       'visible': '',\n",
       "       'children': [{'id': 'input_parameters_text',\n",
       "         'type': 'text',\n",
       "         'text': '1. Source file location (optional, default is Context already loaded): in case you wish to present new source files to use as context,  choose either selecting a folder, file,SAS dataset. pandas DataFrame or a CSV file. Otherwise, provide the name of an existing vector store collection in Configuration.  Note that if choosing a SAS dataset, you must open an input port and attach a table to the custom step.\\n\\n2. Source column ( required if SAS dataset, DataFrame or CSV selected): in case a SAS dataset, pandas DataFrame or a CSV file\\'s selected, users must specify a column within the data source as the main \"document\" source.  The other fields will be considered metadata.\\n\\n3. System prompt (text area, default provided, required): a default system prompt which instructs the LLM on how to handle the question is provided.  Note it makes use of template variables {context} and {question} referring to the context and question respectively.  Edit this system prompt if you\\'d like to change the style of the response.\\n\\n4. Question (text area, required): Provide your question to the LLM. Note that this will be added to additional system prompt, to create a prompt that will be passed to the LLM.',\n",
       "         'visible': ''}]},\n",
       "      {'id': 'parameters_output_specs',\n",
       "       'type': 'section',\n",
       "       'label': 'Output specifications',\n",
       "       'open': False,\n",
       "       'visible': '',\n",
       "       'children': [{'id': 'output_parameters_text',\n",
       "         'type': 'text',\n",
       "         'text': \"Results (the answer from the LLM) are printed by default to the output window.\\n\\n1. Temperature (numeric stepper, default 0): temperature for an LLM affects its abiity to predict the next word when generating responses.  A rule of thumb is that a temperature closer to 0 indicates the model uses the predicted next word with the highest probability, whereas a temperature of 1 increases the randomness with which the model predicts the next word.  \\n\\n2. Context size (numeric stepper, default 10): select how many similar results from the vector store should be retrieved and provided as context to the LLM.  Note that a higher number results in more tokens provided as part of the prompt.\\n\\n3. Output table (output port, option): attach either a CAS table or sas7bdat to the output port of this node to hold results.  These results contain the LLM's answer, the original question and supporting retrieved results.  \",\n",
       "         'visible': ''}]},\n",
       "      {'id': 'parameters_config',\n",
       "       'type': 'section',\n",
       "       'label': 'Configuration ',\n",
       "       'open': 1,\n",
       "       'visible': '',\n",
       "       'children': [{'id': 'output_parameters_text_1',\n",
       "         'type': 'text',\n",
       "         'text': \"1. Embedding model (text field, required):  provide the name of your Azure OpenAI deployment of an OpenAI embedding model. For convenience, it's suggested to use the same name as the model you wish to use. For example, if your OpenAI embedding model happens to be text-embedding-3-small, use the same name for your deployment. \\n\\n2. Vector Store persistent path (text field, defaults to /tmp if blank): provide a path to a ChromaDB database.  If blank, this defaults to /tmp on the filesystem. \\n\\n3. Chroma DB collection name (text field): provide name of the Chroma DB collection you wish to use.  If the collection does not exist, a new one will be created. Ensure you have write access to the persistent area.\\n\\n4. Text generation model (text field, required): provide the name of an Azure OpenAI text generation deployment.  For convenience, you may choose to use the same name as the OpenAI LLM. Example, gpt-35-turbo to gpt-35-turbo.\\n\\n5. Azure OpenAI service details (file selector for key and text fields, required): provide a path to your Azure OpenAI access key.  Ensure this key is saved within a text file in a secure location on the filesystem.  Users are responsible for providing their keys to use this service.  In addition, also refer to your Azure OpenAI service to obtain the service endpoint and region. The OpenAI API version can be changed if required.\",\n",
       "         'visible': ''}]}]},\n",
       "    {'id': 'about_runtimecontrol',\n",
       "     'type': 'section',\n",
       "     'label': 'Run-time Control',\n",
       "     'open': 0,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'runtimecontrol_text',\n",
       "       'type': 'text',\n",
       "       'text': 'Note: Run-time control is optional.  You may choose whether to execute the main code of this step or not, based on upstream conditions set by earlier SAS programs.  This includes nodes run prior to this custom step earlier in a SAS Studio Flow, or a previous program in the same session.\\n\\nRefer this blog (https://communities.sas.com/t5/SAS-Communities-Library/Switch-on-switch-off-run-time-control-of-SAS-Studio-Custom-Steps/ta-p/885526) for more details on the concept.\\n\\nThe following macro variable,\\n\\n_aor_run_trigger\\n\\nwill initialize with a value of 1 by default, indicating an \"enabled\" status and allowing the custom step to run.\\n\\nIf you wish to control execution of this custom step, include code in an upstream SAS program to set this variable to 0.  This \"disables\" execution of the custom step.\\n\\nTo \"disable\" this step, run the following code upstream:\\n\\n%global _aor_run_trigger;\\n%let _aor_run_trigger =0;\\n\\nTo \"enable\" this step again, run the following (it\\'s assumed that this has already been set as a global variable):\\n\\n%let _aor_run_trigger =1;\\n\\nIMPORTANT: Be aware that disabling this step means that none of its main execution code will run, and any  downstream code which was dependent on this code may fail.  Change this setting only if it aligns with the objective of your SAS Studio program.',\n",
       "       'visible': ''}]},\n",
       "    {'id': 'about_documentation',\n",
       "     'type': 'section',\n",
       "     'label': 'Documentation',\n",
       "     'open': 0,\n",
       "     'visible': '',\n",
       "     'children': [{'id': 'documentation_text',\n",
       "       'type': 'text',\n",
       "       'text': '1.  Azure OpenAI service: https://learn.microsoft.com/en-us/azure/ai-services/openai/\\n\\n2. Documentation for the chromadb Python package: https://docs.trychroma.com\\n\\n3.  Documentation for the \"Vector Databases - Hydrate Chroma DB collection\" SAS Studio Custom Step: https://github.com/sassoftware/sas-studio-custom-steps/tree/main/Vector%20Databases%20-%20Hydrate%20Chroma%20DB%20Collection\\n\\n4. An important note regarding sqlite: https://docs.trychroma.com/troubleshooting#sqlite\\n\\n5. SAS Communities article on configuring Viya for Python integration: https://communities.sas.com/t5/SAS-Communities-Library/Configuring-SAS-Viya-for-Python-Integration/ta-p/847459\\n\\n6. The SAS Viya Platform Deployment Guide (refer to SAS Configurator for Open Source within): https://go.documentation.sas.com/doc/en/itopscdc/default/itopssr/p1n66p7u2cm8fjn13yeggzbxcqqg.htm?fromDefault=#p19cpvrrjw3lurn135ih46tjm7oi \\n\\n7.  Options for persistent clients and client connections in Chroma: https://docs.trychroma.com/usage-guide\\n\\n8. Langchain Python documentation: https://python.langchain.com/docs/get_started/introduction\\n\\n9. OpenAI API versions change periodically. Keep track of them here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation',\n",
       "       'visible': ''}]},\n",
       "    {'id': 'version_text',\n",
       "     'type': 'text',\n",
       "     'text': 'Version: 1.3.3  (14NOV2024)',\n",
       "     'visible': ''},\n",
       "    {'id': 'contact_text',\n",
       "     'type': 'text',\n",
       "     'text': 'Created/contact: \\n\\n- Samiul Haque (samiul.haque@sas.com)\\n- Sundaresh Sankaran (sundaresh.sankaran@sas.com)\\n- Renato Luppi (renato.luppi@sas.com)\\n',\n",
       "     'visible': ''}]}],\n",
       " 'values': {'inputData': {'library': '', 'table': ''},\n",
       "  'textCol': [],\n",
       "  'docId': [],\n",
       "  'copyVarList': [],\n",
       "  'systemPrompt': '',\n",
       "  'userPrompt': '',\n",
       "  'temperature': None,\n",
       "  'outputTable': {'library': '', 'table': ''},\n",
       "  'embeddingModelDeployment': '',\n",
       "  'genModelDeployment': '',\n",
       "  'azureKeyLocation': '',\n",
       "  'azureOpenAIEndpoint': 'https://<your_openai_service>.azure.com/',\n",
       "  'azureRegion': 'eastus2',\n",
       "  'openAIVersion': '2024-10-21'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd = json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs[\"ui\"]=jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Auto_Generated_c673e432-415c-4b4e-beba-6a2806364744',\n",
       " 'creationTimeStamp': None,\n",
       " 'modifiedTimeStamp': None,\n",
       " 'createdBy': None,\n",
       " 'modifiedBy': None,\n",
       " 'displayName': None,\n",
       " 'localDisplayName': None,\n",
       " 'properties': None,\n",
       " 'links': None,\n",
       " 'metadataVersion': None,\n",
       " 'version': None,\n",
       " 'type': None,\n",
       " 'flowMetadata': None,\n",
       " 'ui': '{\"showPageContentOnly\": true, \"pages\": [{\"id\": \"params\", \"type\": \"page\", \"label\": \"Parameters\", \"children\": [{\"id\": \"section_input_params\", \"type\": \"section\", \"label\": \"Parameters\", \"open\": true, \"children\": [{\"id\": \"inputData\", \"type\": \"inputtable\", \"label\": \"Select input data\", \"required\": true, \"placeholder\": \"\", \"visible\": \"\"}, {\"id\": \"textCol\", \"type\": \"columnselector\", \"label\": \"Select text column containing instructions and context:\", \"include\": null, \"order\": false, \"columntype\": \"a\", \"max\": null, \"min\": null, \"visible\": \"\", \"table\": null}, {\"id\": \"docId\", \"type\": \"columnselector\", \"label\": \"Select ID column:\", \"include\": null, \"order\": false, \"columntype\": \"a\", \"max\": null, \"min\": null, \"visible\": \"\", \"table\": null}, {\"id\": \"copyVarList\", \"type\": \"columnselector\", \"label\": \"Select additional columns to copy over:\", \"include\": null, \"order\": false, \"columntype\": \"a\", \"max\": null, \"min\": null, \"visible\": \"\", \"table\": null}, {\"id\": \"section_prompts\", \"type\": \"section\", \"label\": \"Prompts\", \"open\": true, \"children\": [{\"id\": \"systemPrompt\", \"type\": \"textarea\", \"label\": \"Provide system prompt:\", \"placeholder\": \"\", \"required\": false, \"visible\": \"\"}, {\"id\": \"userPrompt\", \"type\": \"textarea\", \"label\": \"Provide user prompt:\", \"placeholder\": \"\", \"required\": false, \"visible\": \"\"}]}]}, {\"id\": \"section_output_specs\", \"type\": \"section\", \"label\": \"Output specification\", \"open\": true, \"visible\": \"\", \"children\": [{\"id\": \"temperature\", \"type\": \"numstepper\", \"label\": \"Select temperature for output:\", \"required\": false, \"integer\": false, \"min\": null, \"max\": null, \"stepsize\": 1}, {\"id\": \"outputTable\", \"type\": \"outputtable\", \"label\": \"Provide output table:\", \"required\": true, \"placeholder\": \"\", \"visible\": \"\"}]}]}, {\"id\": \"config\", \"type\": \"page\", \"label\": \"Configuration\", \"children\": [{\"id\": \"embedding_model\", \"type\": \"section\", \"label\": \"Embedding model\", \"open\": true, \"children\": [{\"id\": \"embeddingModelDeployment\", \"type\": \"textfield\", \"label\": \"Provide name of your Azure OpenAI model deployment for embedding models: \", \"placeholder\": \"\", \"required\": true, \"visible\": \"\"}]}, {\"id\": \"section5\", \"type\": \"section\", \"label\": \"Text generation model\", \"open\": false, \"children\": [{\"id\": \"genModelDeployment\", \"type\": \"textfield\", \"label\": \"Provide name of your Azure OpenAI generation model deployment:\", \"placeholder\": \"\", \"required\": true, \"visible\": \"\"}]}, {\"id\": \"section_azure_openai\", \"type\": \"section\", \"label\": \"Azure OpenAI service:\", \"open\": true, \"children\": [{\"id\": \"azureKeyLocation\", \"type\": \"path\", \"label\": \"Provide path to your Azure OpenAI key:\", \"pathtype\": \"file\", \"placeholder\": \"\", \"required\": false, \"visible\": \"\"}, {\"id\": \"text_key_details\", \"type\": \"text\", \"text\": \"Ensure this key is located in a file saved in a secure folder.\", \"visible\": \"\"}, {\"id\": \"azureOpenAIEndpoint\", \"type\": \"textfield\", \"label\": \"Provide URL for Azure OpenAI service endpoint:\", \"placeholder\": \"\", \"required\": true, \"visible\": \"\"}, {\"id\": \"azureRegion\", \"type\": \"textfield\", \"label\": \"Provide region for Azure OpenAI service\", \"placeholder\": \"Default value: eastus2\", \"required\": false, \"visible\": \"\"}, {\"id\": \"openAIVersion\", \"type\": \"textfield\", \"label\": \"OpenAI API Version\", \"placeholder\": \"2024-10-21\", \"required\": false, \"visible\": \"\"}]}]}, {\"id\": \"about\", \"type\": \"page\", \"label\": \"About\", \"children\": [{\"id\": \"about_description\", \"type\": \"text\", \"text\": \"LLM - Azure OpenAI-based Retrieval Augmented Generation (RAG) \\\\n==============================================\\\\n\\\\nThis custom step uses a Retrieval Augmented Generation (RAG) approach to provide right context to an Azure OpenAI Large Language Model (LLM) for answering a question.  \\\\n\\\\nLLMs require relevant context to provide useful answers, especially for questions based on a local corpus of knowledge.  \\\\n\\\\nA RAG approach, explained in simple terms, retrieves relevant data from a knowledge base and provides the same to an LLM to use as context.  RAG-based are expected to reduce LLM hallucinations (i.e. an LLM provides irrelevant or false answers).  This custom step implements RAG with a Chroma DB vector store and passes retrieved documents to an Azure OpenAI service.   \\\\n\\\\n**IMPORTANT:** Be aware that this custom step uses an Azure OpenAI service that results in data being sent over to the service.  Ensure you use this only in accordance with your organization\\'s policies on calling external LLMs.\", \"visible\": \"\"}, {\"id\": \"section_prereqs\", \"type\": \"section\", \"label\": \"Prerequisites\", \"open\": false, \"visible\": \"\", \"children\": [{\"id\": \"text_prereqs\", \"type\": \"text\", \"text\": \"1. Python:  Python version 3.10 is recommended to avoid package support or dependency issues.\\\\n\\\\n2. Python packages to be installed:\\\\n\\\\n   i.    langchain: https://pypi.org/project/langchain/\\\\n   ii.   langchain-community: https://pypi.org/project/langchain-community/\\\\n   iii.  langchain-openai: https://pypi.org/project/langchain-openai/\\\\n  iv.   PyPDF: https://pypi.org/project/pypdf/\\\\n  v.    sentence-transformers: https://pypi.org/project/sentence-transformers/\\\\n  vi.   chromadb: https://pypi.org/project/chromadb/\\\\n  vii.  pysqlite-binary: https://pypi.org/project/pysqlite-binary/\\\\n\\\\n3. Viya 4 environment version 2024.01 or later\\\\n\\\\n4. Valid Azure OpenAI service with embedding & large language models deployed.  Refer here for instructions: https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio \\\\n\", \"visible\": \"\"}]}, {\"id\": \"section_assumptions\", \"type\": \"section\", \"label\": \"Assumptions\", \"open\": false, \"visible\": \"\", \"children\": [{\"id\": \"text_assumptions\", \"type\": \"text\", \"text\": \"Current assumptions for this initial versions (future versions may improve upon the same):\\\\n\\\\n1. Users  choose either an existing Chroma DB vector database collection or load PDF,  SAS dataset, pandas DataFrame or CSV files to an existing or new Chroma DB collection.\\\\n\\\\n2. Users may load all PDFs in a directory on the SAS Server (filesystem), or select a PDF/sas7bdat/DataFrame/CSV of their choice.\\\\n\\\\n3. The code assumes use of a Chroma DB vector store.  Users may choose to replace this with other supported vector stores.\\\\n\\\\n4. The code uses the langchain LLM framework.  \\\\n\\\\n5. PDFs (containing text), CSV, SAS datasets and pandas DataFrames are currently the only loadable file format allowed.  Users are however free to ingest various other document types into a Chroma DB collection beforehand, using the \\\\\"Vector Databases - Hydrate Chroma DB collection\\\\\" SAS Studio Custom Step (refer documentation)\\\\n\\\\n6. User has already configured Azure OpenAI to deploy both an embedding function and LLM service, or knows the deployment names. \\\\n\", \"visible\": \"\"}]}, {\"id\": \"about_parameters\", \"type\": \"section\", \"label\": \"Parameters\", \"open\": true, \"visible\": \"\", \"children\": [{\"id\": \"parameters_input\", \"type\": \"section\", \"label\": \"Input parameters\", \"open\": true, \"visible\": \"\", \"children\": [{\"id\": \"input_parameters_text\", \"type\": \"text\", \"text\": \"1. Source file location (optional, default is Context already loaded): in case you wish to present new source files to use as context,  choose either selecting a folder, file,SAS dataset. pandas DataFrame or a CSV file. Otherwise, provide the name of an existing vector store collection in Configuration.  Note that if choosing a SAS dataset, you must open an input port and attach a table to the custom step.\\\\n\\\\n2. Source column ( required if SAS dataset, DataFrame or CSV selected): in case a SAS dataset, pandas DataFrame or a CSV file\\'s selected, users must specify a column within the data source as the main \\\\\"document\\\\\" source.  The other fields will be considered metadata.\\\\n\\\\n3. System prompt (text area, default provided, required): a default system prompt which instructs the LLM on how to handle the question is provided.  Note it makes use of template variables {context} and {question} referring to the context and question respectively.  Edit this system prompt if you\\'d like to change the style of the response.\\\\n\\\\n4. Question (text area, required): Provide your question to the LLM. Note that this will be added to additional system prompt, to create a prompt that will be passed to the LLM.\", \"visible\": \"\"}]}, {\"id\": \"parameters_output_specs\", \"type\": \"section\", \"label\": \"Output specifications\", \"open\": false, \"visible\": \"\", \"children\": [{\"id\": \"output_parameters_text\", \"type\": \"text\", \"text\": \"Results (the answer from the LLM) are printed by default to the output window.\\\\n\\\\n1. Temperature (numeric stepper, default 0): temperature for an LLM affects its abiity to predict the next word when generating responses.  A rule of thumb is that a temperature closer to 0 indicates the model uses the predicted next word with the highest probability, whereas a temperature of 1 increases the randomness with which the model predicts the next word.  \\\\n\\\\n2. Context size (numeric stepper, default 10): select how many similar results from the vector store should be retrieved and provided as context to the LLM.  Note that a higher number results in more tokens provided as part of the prompt.\\\\n\\\\n3. Output table (output port, option): attach either a CAS table or sas7bdat to the output port of this node to hold results.  These results contain the LLM\\'s answer, the original question and supporting retrieved results.  \", \"visible\": \"\"}]}, {\"id\": \"parameters_config\", \"type\": \"section\", \"label\": \"Configuration \", \"open\": 1, \"visible\": \"\", \"children\": [{\"id\": \"output_parameters_text_1\", \"type\": \"text\", \"text\": \"1. Embedding model (text field, required):  provide the name of your Azure OpenAI deployment of an OpenAI embedding model. For convenience, it\\'s suggested to use the same name as the model you wish to use. For example, if your OpenAI embedding model happens to be text-embedding-3-small, use the same name for your deployment. \\\\n\\\\n2. Vector Store persistent path (text field, defaults to /tmp if blank): provide a path to a ChromaDB database.  If blank, this defaults to /tmp on the filesystem. \\\\n\\\\n3. Chroma DB collection name (text field): provide name of the Chroma DB collection you wish to use.  If the collection does not exist, a new one will be created. Ensure you have write access to the persistent area.\\\\n\\\\n4. Text generation model (text field, required): provide the name of an Azure OpenAI text generation deployment.  For convenience, you may choose to use the same name as the OpenAI LLM. Example, gpt-35-turbo to gpt-35-turbo.\\\\n\\\\n5. Azure OpenAI service details (file selector for key and text fields, required): provide a path to your Azure OpenAI access key.  Ensure this key is saved within a text file in a secure location on the filesystem.  Users are responsible for providing their keys to use this service.  In addition, also refer to your Azure OpenAI service to obtain the service endpoint and region. The OpenAI API version can be changed if required.\", \"visible\": \"\"}]}]}, {\"id\": \"about_runtimecontrol\", \"type\": \"section\", \"label\": \"Run-time Control\", \"open\": 0, \"visible\": \"\", \"children\": [{\"id\": \"runtimecontrol_text\", \"type\": \"text\", \"text\": \"Note: Run-time control is optional.  You may choose whether to execute the main code of this step or not, based on upstream conditions set by earlier SAS programs.  This includes nodes run prior to this custom step earlier in a SAS Studio Flow, or a previous program in the same session.\\\\n\\\\nRefer this blog (https://communities.sas.com/t5/SAS-Communities-Library/Switch-on-switch-off-run-time-control-of-SAS-Studio-Custom-Steps/ta-p/885526) for more details on the concept.\\\\n\\\\nThe following macro variable,\\\\n\\\\n_aor_run_trigger\\\\n\\\\nwill initialize with a value of 1 by default, indicating an \\\\\"enabled\\\\\" status and allowing the custom step to run.\\\\n\\\\nIf you wish to control execution of this custom step, include code in an upstream SAS program to set this variable to 0.  This \\\\\"disables\\\\\" execution of the custom step.\\\\n\\\\nTo \\\\\"disable\\\\\" this step, run the following code upstream:\\\\n\\\\n%global _aor_run_trigger;\\\\n%let _aor_run_trigger =0;\\\\n\\\\nTo \\\\\"enable\\\\\" this step again, run the following (it\\'s assumed that this has already been set as a global variable):\\\\n\\\\n%let _aor_run_trigger =1;\\\\n\\\\nIMPORTANT: Be aware that disabling this step means that none of its main execution code will run, and any  downstream code which was dependent on this code may fail.  Change this setting only if it aligns with the objective of your SAS Studio program.\", \"visible\": \"\"}]}, {\"id\": \"about_documentation\", \"type\": \"section\", \"label\": \"Documentation\", \"open\": 0, \"visible\": \"\", \"children\": [{\"id\": \"documentation_text\", \"type\": \"text\", \"text\": \"1.  Azure OpenAI service: https://learn.microsoft.com/en-us/azure/ai-services/openai/\\\\n\\\\n2. Documentation for the chromadb Python package: https://docs.trychroma.com\\\\n\\\\n3.  Documentation for the \\\\\"Vector Databases - Hydrate Chroma DB collection\\\\\" SAS Studio Custom Step: https://github.com/sassoftware/sas-studio-custom-steps/tree/main/Vector%20Databases%20-%20Hydrate%20Chroma%20DB%20Collection\\\\n\\\\n4. An important note regarding sqlite: https://docs.trychroma.com/troubleshooting#sqlite\\\\n\\\\n5. SAS Communities article on configuring Viya for Python integration: https://communities.sas.com/t5/SAS-Communities-Library/Configuring-SAS-Viya-for-Python-Integration/ta-p/847459\\\\n\\\\n6. The SAS Viya Platform Deployment Guide (refer to SAS Configurator for Open Source within): https://go.documentation.sas.com/doc/en/itopscdc/default/itopssr/p1n66p7u2cm8fjn13yeggzbxcqqg.htm?fromDefault=#p19cpvrrjw3lurn135ih46tjm7oi \\\\n\\\\n7.  Options for persistent clients and client connections in Chroma: https://docs.trychroma.com/usage-guide\\\\n\\\\n8. Langchain Python documentation: https://python.langchain.com/docs/get_started/introduction\\\\n\\\\n9. OpenAI API versions change periodically. Keep track of them here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\", \"visible\": \"\"}]}, {\"id\": \"version_text\", \"type\": \"text\", \"text\": \"Version: 1.3.3  (14NOV2024)\", \"visible\": \"\"}, {\"id\": \"contact_text\", \"type\": \"text\", \"text\": \"Created/contact: \\\\n\\\\n- Samiul Haque (samiul.haque@sas.com)\\\\n- Sundaresh Sankaran (sundaresh.sankaran@sas.com)\\\\n- Renato Luppi (renato.luppi@sas.com)\\\\n\", \"visible\": \"\"}]}], \"values\": {\"inputData\": {\"library\": \"\", \"table\": \"\"}, \"textCol\": [], \"docId\": [], \"copyVarList\": [], \"systemPrompt\": \"\", \"userPrompt\": \"\", \"temperature\": null, \"outputTable\": {\"library\": \"\", \"table\": \"\"}, \"embeddingModelDeployment\": \"\", \"genModelDeployment\": \"\", \"azureKeyLocation\": \"\", \"azureOpenAIEndpoint\": \"https://<your_openai_service>.azure.com/\", \"azureRegion\": \"eastus2\", \"openAIVersion\": \"2024-10-21\"}}',\n",
       " 'templates': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Step created at Test_Zero_Shot.step\n"
     ]
    }
   ],
   "source": [
    "cs.create_custom_step(custom_step_path=\"Test_Zero_Shot.step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
