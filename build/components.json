{
	"showPageContentOnly": true,
	"pages": [
		{
			"id": "params",
			"type": "page",
			"label": "Parameters",
			"children": [
				{
					"id": "section_input_params",
					"type": "section",
					"label": "Parameters",
					"open": true,
					"children": [
						{
							"id": "inputData",
							"type": "inputtable",
							"label": "Select input data:",
							"required": true,
							"placeholder": "",
							"visible": ""
						},
						{
							"id": "textCol",
							"type": "columnselector",
							"label": "Select text column:",
							"include": null,
							"order": false,
							"columntype": "a",
							"max": null,
							"min": null,
							"visible": "",
							"table": "inputData"
						},
						{
							"id": "section_prompts",
							"type": "section",
							"label": "Prompts",
							"open": true,
							"children": [
								{
									"id": "_systemPrompt",
									"type": "textarea",
									"label": "Provide system prompt:",
									"placeholder": "",
									"required": false,
									"visible": ""
								},
								{
									"id": "text_system_prompt",
									"type": "text",
									"text": "Use the system prompt to provide broad instructions to the LLM such as role, task description and response specification.",
									"visible": ""
								},
								{
									"id": "userPrompt",
									"type": "textarea",
									"label": "Provide user prompt:",
									"placeholder": "",
									"required": false,
									"visible": ""
								},
								{
									"id": "text_user_prompt",
									"type": "text",
									"text": "Use the user prompt to provide specific instructions on the task to perform.  Use tags like {Question}, {Context} etc. to refer to the context provided.",
									"visible": ""
								},
								{
									"id": "userExample",
									"type": "textarea",
									"label": "Provide illustrative example(s):",
									"placeholder": "",
									"required": false,
									"visible": ""
								},
								{
									"id": "text_user_example",
									"type": "text",
									"text": "Use the user example to provide 0, 1, or more illustrative examples of context and desired response from the LLM. Tag examples as Example: and Answer:.",
									"visible": ""
								}
							]
						}
					]
				},
				{
					"id": "section_output_specs",
					"type": "section",
					"label": "Output specification",
					"open": true,
					"visible": "",
					"children": [
						{
							"id": "temperature",
							"type": "numstepper",
							"label": "Select temperature for output:",
							"required": false,
							"integer": false,
							"min": 0,
							"max": 1,
							"stepsize": 0.05
						},
						{
							"id": "text_temperature",
							"type": "text",
							"text": "Use the temperature control to govern how the LLM generates its response based on next word prediction.",
							"visible": ""
						},
						{
							"id": "topP",
							"type": "numstepper",
							"label": "Select top P for output:",
							"required": false,
							"integer": false,
							"min": 0,
							"max": 1,
							"stepsize": 0.05
						},
						{
							"id": "text_topP",
							"type": "text",
							"text": "Similar to temperature, this controls how deterministic the model is. Lowering Top P will narrow the modelâ€™s token selection to likelier tokens. Increasing Top P will let the model choose from tokens with both high and low likelihood. Try adjusting temperature or Top P but not both.",
							"visible": ""
						},
						{
							"id": "maxTokens",
							"type": "numstepper",
							"label": "Select the max number of tokens for output:",
							"required": false,
							"integer": true,
							"min": 1,
							"max": null,
							"stepsize": 1
						},
						{
							"id": "text_maxTokens",
							"type": "text",
							"text": "Use the max token control to limit the maximum number of tokens per the response. 1 token is approximately 4 characters of English text.",
							"visible": ""
						},
						{
							"id": "frequencyPenalty",
							"type": "numstepper",
							"label": "Select the frequency penalty:",
							"required": false,
							"integer": false,
							"min": -2,
							"max": 2,
							"stepsize": 0.05
						},
						{
							"id": "text_frequencyPenalty",
							"type": "text",
							"text": "The frequency penalty reduces the likelihood of repeating words by applying a penalty to tokens based on how often they have already appeared in the text. A higher frequency penalty decreases repetition by discouraging the model from generating the same words multiple times.",
							"visible": ""
						},
						{
							"id": "presencePenalty",
							"type": "numstepper",
							"label": "Select the presence penalty:",
							"required": false,
							"integer": false,
							"min": -2,
							"max": 2,
							"stepsize": 0.05
						},
						{
							"id": "text_presencePenalty",
							"type": "text",
							"text": "The presence penalty reduces the chance of repeating any word that has already appeared in the text, encouraging the model to introduce new topics. Unlike the frequency penalty, it applies a uniform penalty to all repeated tokens, regardless of how often they appear.",
							"visible": ""
						},
						{
							"id": "outputTable",
							"type": "outputtable",
							"label": "Provide output table:",
							"required": true,
							"placeholder": "",
							"visible": ""
						}
					]
				}
			]
		},
		{
			"id": "config",
			"type": "page",
			"label": "Configuration",
			"children": [
				{
					"id": "section_gen_model",
					"type": "section",
					"label": "Text generation model",
					"open": true,
					"children": [
						{
							"id": "genModelDeployment",
							"type": "textfield",
							"label": "Provide name of your Azure OpenAI generation model deployment:",
							"placeholder": "",
							"required": true,
							"visible": ""
						}
					]
				},
				{
					"id": "section_azure_openai",
					"type": "section",
					"label": "Azure OpenAI service:",
					"open": true,
					"children": [
						{
							"id": "azureKeyLocation",
							"type": "path",
							"label": "Provide path to your Azure OpenAI key:",
							"pathtype": "file",
							"placeholder": "",
							"required": false,
							"visible": ""
						},
						{
							"id": "text_key_details",
							"type": "text",
							"text": "Ensure this key is located in a file saved in a secure folder.",
							"visible": ""
						},
						{
							"id": "azureOpenAIEndpoint",
							"type": "textfield",
							"label": "Provide URL for Azure OpenAI service endpoint:",
							"placeholder": "",
							"required": true,
							"visible": ""
						},
						{
							"id": "azureRegion",
							"type": "textfield",
							"label": "Provide region for Azure OpenAI service",
							"placeholder": "Default value: eastus2",
							"required": false,
							"visible": ""
						},
						{
							"id": "openAIVersion",
							"type": "textfield",
							"label": "OpenAI API Version",
							"placeholder": "2024-10-21",
							"required": false,
							"visible": ""
						}
					]
				}
			]
		},
		{
			"id": "about",
			"type": "page",
			"label": "About",
			"children": [
				{
					"id": "about_description",
					"type": "text",
					"text": "LLM - Azure OpenAI In-context Learning==============================================This custom step helps you interact with a Large Language Model (LLM) calling an [Azure OpenAI](https://microsoftlearning.github.io/mslearn-openai/Instructions/Exercises/01-get-started-azure-openai.html) service to process simple instructions on specified input data. It uses an approach called In-context learning which uses provided examples to perform a task.  If no example is provided, then the LLM simply uses the provided context.  This is useful for cases where a call to an LLM does not require prior search, filter or query of data sources (such as what Retrieval Augmented Generation provides) . Run inside a SAS session, this custom step takes either a SAS dataset or a CAS table as input and returns a SAS dataset (or CAS table) as output, with the response added as a new variable.",
					"visible": ""
				},
				{
					"id": "section_prereqs",
					"type": "section",
					"label": "Prerequisites",
					"open": false,
					"visible": "",
					"children": [
						{
							"id": "text_prereqs",
							"type": "text",
							"text": "1. Python is available to the SAS Viya Compute session.  \n2. Python packages to be installed:\n     i.  openai: https://pypi.org/project/openai/\n     ii.  pandas: https://pypi.org/project/pandas/\n3. Viya 4 environment version 2025.01 or later \n4. Valid Azure OpenAI service with large language models deployed.  Refer here for instructions: https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-studio",
							"visible": ""
						}
					]
				},
				{
					"id": "about_parameters",
					"type": "section",
					"label": "Parameters",
					"open": true,
					"visible": "",
					"children": [
						{
							"id": "parameters_input",
							"type": "section",
							"label": "Input parameters",
							"open": true,
							"visible": "",
							"children": [
								{
									"id": "input_parameters_text",
									"type": "text",
									"text": "1. Input table (input port, required): Attach either a SAS dataset or a SAS Cloud Analytics Services (CAS) table to the input port of this step.\n2. Select text column (column selector, max 1): Select a column containing the context to be provided to the LLM\n3. System prompt (text area): Provide a system prompt for use by the LLM. The system prompt helps clarify the role that the LLM plays and is also used for other instructions such as how to answer the question and the format in which to present the results.\n4. User prompt (text area): Provide a user prompt which contains specific instructions to be fulfilled by the LLM.  In the user prompt, use fields like {Question} and {Context} to refer to the question or context provided.\n5. Illustrative examples (text area):  Provide some illustrative examples (the 'shots') which are used as a basis for the LLM to answer the question. Provide this in the form of \"Question:\" and \"Answer:\" pairs",
									"visible": ""
								}
							]
						},
						{
							"id": "parameters_output_specs",
							"type": "section",
							"label": "Output specifications",
							"open": false,
							"visible": "",
							"children": [
								{
									"id": "output_parameters_text",
									"type": "text",
									"text": "1. Temperature (numeric stepper): Use the temperature control to govern how the LLM generates its response based on next word prediction.\n     i. Recommendation: Try adjusting Temperature or Top p, but not both.\n2. Top P (numeric stepper): Use the top p control to govern randomness using nucleus sampling.  \n    i. Recommendation: Try adjusting Temperature or Top p, but not both.\n3. Max Tokens (numeric stepper): Use the max token control to limit the maximum number of tokens per the response. 1 token is approximately 4 characters of English text.\n4. Frequency Penalty (numeric stepper): The frequency penalty reduces the likelihood of repeating words by applying a penalty to tokens based on how often they have already appeared in the text. A higher frequency penalty decreases repetition by discouraging the model from generating the same words multiple times.\n5. Presence Penalty (numeric stepper): The presence penalty reduces the chance of repeating any word that has already appeared in the text, encouraging the model to introduce new topics. Unlike the frequency penalty, it applies a uniform penalty to all repeated tokens, regardless of how often they appear.\n6. Output table (output port, required) : Attach a table referring to either a SAS dataset or CAS table which contains the original input columns along with the LLM's answer.",
									"visible": "",
									"indent": 0
								}
							]
						},
						{
							"id": "parameters_config",
							"type": "section",
							"label": "Configuration ",
							"open": 1,
							"visible": "",
							"children": [
								{
									"id": "output_parameters_text_1",
									"type": "text",
									"text": "1. Text generation model (text field): Provide the name of a Azure OpenAI model deployment.For convenience, you may choose to use the same name as the OpenAI LLM. Example, gpt-35-turbo to gpt-35-turbo.\n2. Azure Key Location (file selector): Provide path to your Azure OpenAI key, located on the filesystem.  This would be a text file containing the value of the key.  Make sure this is saved in a secure location.\n3. Azure OpenAI endpoint (text field): Provide the URL of the Azure OpenAI service.\n4. Azure OpenAI region (text field): Provide the region where the service is set up (e.g. eastus2)\n5. OpenAI version (default provided): Provide an OpenAI version number in case you want to override the default.",
									"visible": ""
								}
							]
						}
					]
				},
				{
					"id": "section_assumptions",
					"type": "section",
					"label": "Assumptions",
					"open": false,
					"visible": "",
					"children": [
						{
							"id": "text_assumptions",
							"type": "text",
							"text": "Current assumptions for this initial versions (future versions may improve upon the same):\n1. Users  choose either a SAS dataset or Cloud Analytics Services (CAS) table as their input \n2. User has already configured Azure OpenAI to deploy both an embedding function and LLM service, or knows the deployment names.",
							"visible": ""
						}
					]
				},
				{
					"id": "about_runtimecontrol",
					"type": "section",
					"label": "Run-time Control",
					"open": 0,
					"visible": "",
					"children": [
						{
							"id": "runtimecontrol_text",
							"type": "text",
							"text": "Note: Run-time control is optional.  You may choose whether to execute the main code of this step or not, based on upstream conditions set by earlier SAS programs.  This includes nodes run prior to this custom step earlier in a SAS Studio Flow, or a previous program in the same session.\n\nRefer this blog (https://communities.sas.com/t5/SAS-Communities-Library/Switch-on-switch-off-run-time-control-of-SAS-Studio-Custom-Steps/ta-p/885526) for more details on the concept.\nThe following macro variable,\n```sas\n_aicl_run_trigger\n```\nwill initialize with a value of 1 by default, indicating an 'enabled' status and allowing the custom step to run.\nIf you wish to control execution of this custom step, include code in an upstream SAS program to set this variable to 0.  This 'disables' execution of the custom step.\nTo 'disable' this step, run the following code upstream:\n```sas\n%global _aicl_run_trigger;\n%let _aicl_run_trigger = 0;\n```\nTo 'enable' this step again, run the following (it's assumed that this has already been set as a global variable):\n```sas\n%let _aicl_run_trigger = 1;\n```\n\nIMPORTANT: Be aware that disabling this step means that none of its main execution code will run, and any  downstream code which was dependent on this code may fail.  Change this setting only if it aligns with the objective of your SAS Studio program.",
							"visible": ""
						}
					]
				},
				{
					"id": "about_documentation",
					"type": "section",
					"label": "Documentation",
					"open": 0,
					"visible": "",
					"children": [
						{
							"id": "documentation_text",
							"type": "text",
							"text": "1.  Azure OpenAI service: https://learn.microsoft.com/en-us/azure/ai-services/openai/\n2.  SAS Communities article on configuring Viya for Python integration: https://communities.sas.com/t5/SAS-Communities-Library/Configuring-SAS-Viya-for-Python-Integration/ta-p/847459\n3. The SAS Viya Platform Deployment Guide (refer to SAS Configurator for Open Source within): https://go.documentation.sas.com/doc/en/itopscdc/default/itopssr/p1n66p7u2cm8fjn13yeggzbxcqqg.htm?fromDefault=#p19cpvrrjw3lurn135ih46tjm7oi \n4. OpenAI API versions change periodically. Keep track of them here: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation",
							"visible": ""
						}
					]
				},
				{
					"id": "version_text",
					"type": "text",
					"text": "Version: 1.0  (24MAR2025)",
					"visible": ""
				},
				{
					"id": "contact_text",
					"type": "text",
					"text": "Created/contact: \n- Sundaresh Sankaran (sundaresh.sankaran@sas.com) \n- Crystal Baker (crystal.baker@sas.com)",
					"visible": ""
				}
			]
		}
	],
	"values": {
		"inputData": {
			"library": "",
			"table": ""
		},
		"textCol": [],
		"_systemPrompt": "",
		"userPrompt": "",
		"temperature": null,
		"topP": null,
		"maxTokens": null,
		"frequencyPenalty": null,
		"presencePenalty": null,
		"outputTable": {
			"library": "",
			"table": ""
		},
		"genModelDeployment": "",
		"azureKeyLocation": "",
		"azureOpenAIEndpoint": "https://<your_openai_service>.azure.com/",
		"azureRegion": "eastus2",
		"openAIVersion": "2024-10-21"
	}
}